import base64
import json
import re
from pathlib import Path
import httpx
from PIL import Image
import numpy as np

BASE_URL = "https://api.a-bly.com/api/v2/screens/SUB_CATEGORY_DEPARTMENT/"
REVIEW_API_URL = "https://api.a-bly.com/api/v2/goods/{sno}/review_summary/"
LEGAL_NOTICE_API_URL = "https://api.a-bly.com/api/v2/goods/{sno}/legal_notice/"
DETAIL_API_URL = "https://api.a-bly.com/api/v3/goods/{sno}/detail/"
OPTIONS_API_URL = "https://api.a-bly.com/api/v2/goods/{sno}/options/"
BASIC_API_URL = "https://api.a-bly.com/api/v3/goods/{sno}/basic/"

OUTPUT_DIR = Path("output")
IMAGES_DIR = OUTPUT_DIR / "images"

MIN_PURCHASE_COUNT = 2000
MIN_REVIEW_COUNT = 100
MIN_POSITIVE_PERCENT = 95
MAX_PRODUCTS = 10

CATEGORIES = {
    "ìì¼“": 293,
    "ì½”íŠ¸": 294,
    "íŒ¨ë”©": 295,
    "ì í¼": 296,
    "ê°€ë””ê±´": 297,
}

HEADERS = {
    "accept": "application/json, text/plain, */*",
    "accept-language": "ko,en-US;q=0.9,en;q=0.8,ja;q=0.7",
    "cache-control": "no-cache",
    "dnt": "1",
    "origin": "https://m.a-bly.com",
    "pragma": "no-cache",
    "referer": "https://m.a-bly.com/",
    "sec-fetch-dest": "empty",
    "sec-fetch-mode": "cors",
    "sec-fetch-site": "same-site",
    "user-agent": "Mozilla/5.0 (iPhone; CPU iPhone OS 18_5 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/18.5 Mobile/15E148 Safari/604.1",
    "x-anonymous-token": "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJhbm9ueW1vdXNfaWQiOiI4MDc4MDMxODkiLCJpYXQiOjE3NjgwNjg2MDV9.VLJgodKMn0Mkounf6APU887rLZQAgYWvWy1hRVB3aFE",
    "x-app-version": "0.1.0",
    "x-device-id": "99e795d7-a1b1-44da-b2b5-263f1743b0a2",
    "x-device-type": "MobileWeb",
    "x-web-type": "Web",
}


def create_initial_token(category_sno: int) -> str:
    payload = {
        "l": "DepartmentCategoryRealtimeRankGenerator",
        "p": {"department_type": "CATEGORY", "category_sno": category_sno},
        "d": "CATEGORY",
        "previous_screen_name": "OVERVIEW",
        "category_sno": category_sno,
    }
    return base64.b64encode(json.dumps(payload, ensure_ascii=False).encode()).decode()


def build_product_url(sno: int) -> str:
    return f"https://m.a-bly.com/goods/{sno}"


def extract_products_from_response(data: dict) -> list[dict]:
    products = []
    for component in data.get("components", []):
        item_list = component.get("entity", {}).get("item_list", [])
        for item in item_list:
            if item.get("type") != "GOODS_CARD":
                continue
            item_entity = item.get("item_entity", {})
            item_data = item_entity.get("item", {})
            products.append(
                {
                    "sno": item_data.get("sno"),
                    "name": item_data.get("name"),
                    "sell_count": item_data.get("sell_count", 0),
                    "price": item_data.get("price"),
                    "market_name": item_data.get("market_name"),
                }
            )
    return products


def fetch_review_info(client: httpx.Client, sno: int) -> dict | None:
    try:
        response = client.get(REVIEW_API_URL.format(sno=sno))
        response.raise_for_status()
        data = response.json()
        review = data.get("review", {})
        return {
            "count": review.get("count", 0),
            "positive_percent": review.get("positive_percent", 0),
        }
    except Exception as e:
        print(f"  [!] ë¦¬ë·° ì¡°íšŒ ì‹¤íŒ¨ (sno={sno}): {e}")
        return None


def fetch_color_info(client: httpx.Client, sno: int) -> str | None:
    """ê³µì‹œì •ë³´ APIì—ì„œ ìƒ‰ìƒ ì •ë³´ ê°€ì ¸ì˜¤ê¸°"""
    try:
        response = client.get(LEGAL_NOTICE_API_URL.format(sno=sno))
        response.raise_for_status()
        data = response.json()
        return data.get("color_md")
    except Exception as e:
        print(f"  [!] ìƒ‰ìƒ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨ (sno={sno}): {e}")
        return None


def fetch_legal_notice_meta(client: httpx.Client, sno: int) -> dict:
    """ê³µì‹œì •ë³´ APIì—ì„œ ë©”íƒ€(ì†Œì¬/ì œì¡°êµ­ ë“±) ê°€ì ¸ì˜¤ê¸°"""
    try:
        response = client.get(LEGAL_NOTICE_API_URL.format(sno=sno))
        response.raise_for_status()
        data = response.json()
        return {
            "color_md": data.get("color_md"),
            "fabric": data.get("fabric"),
            "country": data.get("country"),
        }
    except Exception as e:
        print(f"  [!] ê³µì‹œì •ë³´ ì¡°íšŒ ì‹¤íŒ¨ (sno={sno}): {e}")
        return {}


def fetch_basic_meta(client: httpx.Client, sno: int) -> dict:
    """ê¸°ë³¸ì •ë³´ APIì—ì„œ ê°€ê²©/ì¸ë„¤ì¼(cover_images) ê°€ì ¸ì˜¤ê¸°"""
    try:
        response = client.get(BASIC_API_URL.format(sno=sno))
        response.raise_for_status()
        data = response.json()
        goods = data.get("goods", {})
        price_info = goods.get("price_info", {}) or {}
        cover_images = goods.get("cover_images", []) or []
        # cover_imagesëŠ” URL ë¦¬ìŠ¤íŠ¸
        cover_images = [
            u for u in cover_images if isinstance(u, str) and u.startswith("http")
        ]
        return {
            "price_info": {
                "consumer": price_info.get("consumer"),
                "thumbnail_price": price_info.get("thumbnail_price"),
                "discount_rate": price_info.get("discount_rate"),
            },
            "cover_images": cover_images,
        }
    except Exception as e:
        print(f"  [!] ê¸°ë³¸ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨ (sno={sno}): {e}")
        return {}


def fetch_option_colors(client: httpx.Client, sno: int) -> list[str]:
    """ì˜µì…˜ ì •ë³´ APIì—ì„œ 'ì»¬ëŸ¬' ì˜µì…˜ ê°’ ê°€ì ¸ì˜¤ê¸°"""
    try:
        response = client.get(OPTIONS_API_URL.format(sno=sno), params={"depth": "1"})
        response.raise_for_status()
        data = response.json()

        # ì‘ë‹µ ì˜ˆì‹œëŠ” { "name": "ì»¬ëŸ¬", "option_components": [...] } í˜•íƒœ
        option_name = data.get("name")
        # ì¼€ì´ìŠ¤: "ì»¬ëŸ¬" / "ìƒ‰ìƒ" / "Color" ë“±
        if option_name not in ("ì»¬ëŸ¬", "ìƒ‰ìƒ", "Color", "COLOR"):
            return []

        colors: list[str] = []
        for opt in data.get("option_components", []):
            name = opt.get("name")
            if isinstance(name, str) and name.strip():
                colors.append(name.strip())

        # ì¤‘ë³µ ì œê±°(ìˆœì„œ ìœ ì§€)
        seen = set()
        unique: list[str] = []
        for c in colors:
            if c not in seen:
                seen.add(c)
                unique.append(c)
        return unique
    except Exception as e:
        print(f"  [!] ì˜µì…˜ ìƒ‰ìƒ ì¡°íšŒ ì‹¤íŒ¨ (sno={sno}): {e}")
        return []


def clean_image_url(url: str) -> str | None:
    """ì´ë¯¸ì§€ URL ì •ë¦¬ (HTML ì´ìŠ¤ì¼€ì´í”„ ì²˜ë¦¬)"""
    # HTML ì—”í‹°í‹° ë””ì½”ë”©
    url = url.replace("&quot;", "").replace("\\&quot;", "")
    url = url.replace("&amp;", "&")
    url = url.strip('"').strip("'").strip()

    # ìœ íš¨í•œ URLì¸ì§€ í™•ì¸
    if url.startswith("http://") or url.startswith("https://"):
        return url
    return None


def fetch_detail_images(client: httpx.Client, sno: int) -> list[str]:
    """ìƒì„¸í˜ì´ì§€ APIì—ì„œ ì´ë¯¸ì§€ URL ì¶”ì¶œ"""
    try:
        response = client.get(DETAIL_API_URL.format(sno=sno), params={"channel": "0"})
        response.raise_for_status()
        data = response.json()

        images = []
        goods = data.get("goods", {})

        # detail_html_partsì—ì„œ ì´ë¯¸ì§€ URL ì¶”ì¶œ
        for part in goods.get("detail_html_parts", []):
            if part.get("html_part_type") == "DESCRIPTION":
                for content in part.get("contents", []):
                    # img src ì¶”ì¶œ (ë‹¤ì–‘í•œ ì¸ìš©ë¶€í˜¸ íŒ¨í„´ ì²˜ë¦¬)
                    patterns = [
                        r'<img[^>]+src="([^"]+)"',
                        r"<img[^>]+src='([^']+)'",
                        r'<img[^>]+src=\\"([^\\]+)\\"',
                        r"<img[^>]+src=\\&quot;([^&]+)\\&quot;",
                    ]
                    for pattern in patterns:
                        img_urls = re.findall(pattern, content)
                        images.extend(img_urls)

        # ì¤‘ë³µ ì œê±° ë° URL ì •ë¦¬
        unique_images = []
        seen = set()
        for url in images:
            cleaned = clean_image_url(url)
            if cleaned and cleaned not in seen:
                seen.add(cleaned)
                unique_images.append(cleaned)

        return unique_images
    except Exception as e:
        print(f"  [!] ìƒì„¸ ì´ë¯¸ì§€ ì¡°íšŒ ì‹¤íŒ¨ (sno={sno}): {e}")
        return []


def download_image(client: httpx.Client, url: str, save_path: Path) -> bool:
    """ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ"""
    try:
        response = client.get(url, follow_redirects=True)
        response.raise_for_status()
        save_path.write_bytes(response.content)
        return True
    except Exception as e:
        print(f"    [!] ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
        return False


def download_cover_images(
    client: httpx.Client, sno: int, cover_images: list[str]
) -> list[str]:
    """basic APIì˜ cover_images(ì¸ë„¤ì¼/ëŒ€í‘œì´ë¯¸ì§€) ë‹¤ìš´ë¡œë“œ"""
    if not cover_images:
        return []

    product_dir = IMAGES_DIR / str(sno)
    product_dir.mkdir(parents=True, exist_ok=True)

    downloaded: list[str] = []
    for idx, url in enumerate(cover_images, 1):
        ext = "jpg"
        low = url.lower()
        if ".png" in low:
            ext = "png"
        elif ".webp" in low:
            ext = "webp"
        elif ".gif" in low:
            ext = "gif"

        save_path = product_dir / f"cover_{idx:02d}.{ext}"
        if download_image(client, url, save_path):
            downloaded.append(str(save_path))

    return downloaded


def find_split_points(
    image: Image.Image, threshold: float = 0.98, min_gap: int = 50
) -> list[int]:
    """ì´ë¯¸ì§€ì—ì„œ ë¶„í•  ì§€ì  ì°¾ê¸° (ê· ì¼í•œ ìƒ‰ìƒì˜ ê°€ë¡œì¤„ ê°ì§€)"""
    img_array = np.array(image.convert("RGB"))
    height, width, _ = img_array.shape

    # ê° í–‰ì˜ í”½ì…€ í‘œì¤€í¸ì°¨ ê³„ì‚° (ë‚®ìœ¼ë©´ ê· ì¼í•œ ìƒ‰ìƒ)
    row_std = np.std(img_array, axis=(1, 2))

    # í‘œì¤€í¸ì°¨ê°€ ë‚®ì€ í–‰ ì°¾ê¸° (ê· ì¼í•œ ìƒ‰ìƒ = êµ¬ë¶„ì„ )
    max_std = np.max(row_std)
    if max_std == 0:
        return []
    uniform_rows = row_std < (max_std * (1 - threshold))

    # ì—°ì†ëœ ê· ì¼ í–‰ ê·¸ë£¹ ì°¾ê¸°
    split_points = []
    in_uniform = False
    start = 0

    for i, is_uniform in enumerate(uniform_rows):
        if is_uniform and not in_uniform:
            start = i
            in_uniform = True
        elif not is_uniform and in_uniform:
            mid = (start + i) // 2
            if mid > min_gap and mid < height - min_gap:
                if not split_points or mid - split_points[-1] > min_gap:
                    split_points.append(mid)
            in_uniform = False

    return split_points


def split_image(image_path: Path, min_height: int = 100) -> list[Path]:
    """ì´ë¯¸ì§€ë¥¼ ë¶„í• í•˜ê³  ì €ì¥ (ì›ë³¸ íŒŒì¼ ëŒ€ì²´)"""
    try:
        image = Image.open(image_path)
    except Exception:
        return [image_path]

    width, height = image.size

    # ì„¸ë¡œë¡œ ê¸´ ì´ë¯¸ì§€ê°€ ì•„ë‹ˆë©´ ë¶„í•  ë¶ˆí•„ìš”
    if height < width * 1.5:
        return [image_path]

    split_points = find_split_points(image)

    if not split_points:
        return [image_path]

    # ë¶„í•  ì§€ì ìœ¼ë¡œ ì´ë¯¸ì§€ ìë¥´ê¸°
    saved_paths = []
    points = [0] + split_points + [height]
    stem = image_path.stem
    suffix = image_path.suffix.lower()
    parent = image_path.parent

    for i in range(len(points) - 1):
        top = points[i]
        bottom = points[i + 1]

        if bottom - top < min_height:
            continue

        cropped = image.crop((0, top, width, bottom))

        # JPEGëŠ” RGBA ì§€ì› ì•ˆí•¨ â†’ RGBë¡œ ë³€í™˜
        if suffix in [".jpg", ".jpeg"] and cropped.mode == "RGBA":
            # í°ìƒ‰ ë°°ê²½ì— í•©ì„±
            background = Image.new("RGB", cropped.size, (255, 255, 255))
            background.paste(cropped, mask=cropped.split()[3])
            cropped = background

        output_path = parent / f"{stem}_{i + 1:02d}{suffix}"
        cropped.save(output_path)
        saved_paths.append(output_path)

    # ë¶„í•  ì„±ê³µì‹œ ì›ë³¸ ì‚­ì œ
    if len(saved_paths) > 1:
        image_path.unlink()

    return saved_paths


def download_product_images(client: httpx.Client, product: dict) -> list[str]:
    """ìƒí’ˆì˜ ìƒì„¸ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ë° ë¶„í• """
    sno = product["sno"]
    product_dir = IMAGES_DIR / str(sno)
    product_dir.mkdir(parents=True, exist_ok=True)

    images = fetch_detail_images(client, sno)
    all_images = []

    for idx, url in enumerate(images):
        ext = "jpg"
        if ".png" in url.lower():
            ext = "png"
        elif ".gif" in url.lower():
            ext = "gif"
        elif ".webp" in url.lower():
            ext = "webp"

        filename = f"{idx + 1:03d}.{ext}"
        save_path = product_dir / filename

        if download_image(client, url, save_path):
            # ì´ë¯¸ì§€ ë¶„í•  ì‹œë„
            split_paths = split_image(save_path)
            if len(split_paths) > 1:
                print(f"    âœ‚ï¸ {filename} â†’ {len(split_paths)}ê°œë¡œ ë¶„í• ")
            all_images.extend(str(p) for p in split_paths)

    return all_images


def write_product_metadata(product: dict) -> None:
    """ë¶„ë¥˜ ë‹¨ê³„ì—ì„œ ì‚¬ìš©í•  ìƒí’ˆ ë©”íƒ€ë°ì´í„°ë¥¼ ì´ë¯¸ì§€ í´ë”ì— ì €ì¥"""
    sno = product["sno"]
    product_dir = IMAGES_DIR / str(sno)
    product_dir.mkdir(parents=True, exist_ok=True)

    meta = {
        "sno": sno,
        "name": product.get("name"),
        "category": product.get("category"),
        "market_name": product.get("market_name"),
        "url": product.get("url"),
        # ì˜µì…˜ ê¸°ë°˜ ìƒ‰ìƒ(ì •ë‹µ ì†ŒìŠ¤)
        "option_colors": product.get("option_colors") or [],
        # ì°¸ê³ : ê³µì‹œì •ë³´ ìƒ‰ìƒ(ì½¤ë§ˆ ë¬¸ìì—´ì¼ ìˆ˜ ìˆìŒ)
        "legal_notice_colors": product.get("colors"),
        # ê°€ê²© ì •ë³´ (basic)
        "price_info": product.get("price_info"),
        # ì†Œì¬/ì œì¡°êµ­ (legal_notice)
        "fabric": product.get("fabric"),
        "country": product.get("country"),
        # ì¸ë„¤ì¼ URL (basic)
        "cover_images": product.get("cover_images") or [],
        # ì°¸ê³ ìš©
        "sell_count": product.get("sell_count"),
        "review_count": product.get("review_count"),
        "positive_percent": product.get("positive_percent"),
    }

    meta_path = product_dir / "meta.json"
    meta_path.write_text(
        json.dumps(meta, ensure_ascii=False, indent=2), encoding="utf-8"
    )


def fetch_products_by_category(
    category_sno: int, category_name: str = ""
) -> list[dict]:
    found_products: list[dict] = []
    checked_snos: set[int] = set()
    next_token = create_initial_token(category_sno)

    print(f"\n{'=' * 50}")
    print(f"ì¹´í…Œê³ ë¦¬: {category_name} (sno={category_sno})")
    print(f"{'=' * 50}")

    with httpx.Client(headers=HEADERS, timeout=30) as client:
        while len(found_products) < MAX_PRODUCTS:
            params = {
                "next_token": next_token,
                "category_list[]": str(category_sno),
                "sorting_type": "POPULAR",
            }

            response = client.get(BASE_URL, params=params)
            response.raise_for_status()
            data = response.json()

            products = extract_products_from_response(data)

            for product in products:
                sno = product["sno"]
                if sno in checked_snos:
                    continue
                checked_snos.add(sno)

                if product["sell_count"] < MIN_PURCHASE_COUNT:
                    continue

                print(
                    f"ê²€ì‚¬ì¤‘: {product['name'][:40]}... ({product['sell_count']:,}ê°œ êµ¬ë§¤)"
                )

                review = fetch_review_info(client, sno)
                if not review:
                    continue

                if review["count"] < MIN_REVIEW_COUNT:
                    print(
                        f"  âŒ ë¦¬ë·° ìˆ˜ ë¶€ì¡±: {review['count']}ê°œ < {MIN_REVIEW_COUNT}ê°œ"
                    )
                    continue

                if review["positive_percent"] < MIN_POSITIVE_PERCENT:
                    print(
                        f"  âŒ ê¸ì •ë¥  ë¶€ì¡±: {review['positive_percent']}% < {MIN_POSITIVE_PERCENT}%"
                    )
                    continue

                product["url"] = build_product_url(sno)
                product["review_count"] = review["count"]
                product["positive_percent"] = review["positive_percent"]
                product["category"] = category_name
                found_products.append(product)

                print(
                    f"  âœ… [{len(found_products)}/{MAX_PRODUCTS}] ë¦¬ë·° {review['count']}ê°œ, ê¸ì •ë¥  {review['positive_percent']}%"
                )

                if len(found_products) >= MAX_PRODUCTS:
                    break

            if len(found_products) >= MAX_PRODUCTS:
                break

            next_token = data.get("next_token")
            if not next_token:
                print("No more pages")
                break

    return found_products


def enrich_product_details(products: list[dict]) -> None:
    """ìƒí’ˆ ìƒì„¸ ì •ë³´(ìƒ‰ìƒ, ì´ë¯¸ì§€) ì¶”ê°€"""
    print(f"\n{'=' * 50}")
    print("ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ ì¤‘...")
    print(f"{'=' * 50}")

    with httpx.Client(headers=HEADERS, timeout=60) as client:
        for i, product in enumerate(products, 1):
            sno = product["sno"]
            print(f"\n[{i}/{len(products)}] {product['name'][:40]}...")

            # ê³µì‹œì •ë³´(ìƒ‰ìƒ/ì†Œì¬/ì œì¡°êµ­)
            legal = fetch_legal_notice_meta(client, sno)
            product["colors"] = legal.get("color_md")
            product["fabric"] = legal.get("fabric")
            product["country"] = legal.get("country")
            if product.get("colors"):
                print(f"  ğŸ¨ ìƒ‰ìƒ(ê³µì‹œ): {product['colors']}")
            if product.get("fabric"):
                print(f"  ğŸ§µ ì†Œì¬(í˜¼ìš©ë¥ ): {product['fabric']}")
            if product.get("country"):
                print(f"  ğŸŒ ì œì¡°êµ­: {product['country']}")

            # ì˜µì…˜(ì»¬ëŸ¬) ì •ë³´: ë¶„ë¥˜ì˜ ì •ë‹µ ì†ŒìŠ¤
            option_colors = fetch_option_colors(client, sno)
            product["option_colors"] = option_colors
            if option_colors:
                print(f"  ğŸ¨ ì˜µì…˜ ìƒ‰ìƒ: {', '.join(option_colors)}")

            # ê¸°ë³¸ì •ë³´(ê°€ê²©/ì¸ë„¤ì¼)
            basic = fetch_basic_meta(client, sno)
            product["price_info"] = basic.get("price_info")
            product["cover_images"] = basic.get("cover_images") or []
            if product.get("price_info"):
                pi = product["price_info"] or {}
                cp = pi.get("consumer")
                tp = pi.get("thumbnail_price")
                dr = pi.get("discount_rate")
                msg = []
                if tp is not None:
                    msg.append(f"{tp:,}ì›")
                if dr is not None:
                    msg.append(f"{dr}%")
                if cp is not None:
                    msg.append(f"(ì •ê°€ {cp:,}ì›)")
                if msg:
                    print(f"  ğŸ’° ê°€ê²©: {' '.join(msg)}")

            # ì¸ë„¤ì¼(cover_images) ë‹¤ìš´ë¡œë“œ
            cover_local = download_cover_images(
                client, sno, product.get("cover_images") or []
            )
            product["cover_images_local"] = cover_local
            if cover_local:
                print(f"  ğŸ–¼ï¸ ì¸ë„¤ì¼: {len(cover_local)}ê°œ ë‹¤ìš´ë¡œë“œ")

            # ë¶„ë¥˜ ë‹¨ê³„ì—ì„œ ì‚¬ìš©í•  ë©”íƒ€ë°ì´í„° ì €ì¥
            write_product_metadata(product)

            # ìƒì„¸ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ
            downloaded = download_product_images(client, product)
            product["images"] = downloaded
            print(f"  ğŸ“· ì´ë¯¸ì§€: {len(downloaded)}ê°œ ë‹¤ìš´ë¡œë“œ")


def save_results(products: list[dict]) -> None:
    """ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
    output_file = OUTPUT_DIR / "products.json"

    # ì´ë¯¸ì§€ ê²½ë¡œë¥¼ ìƒëŒ€ ê²½ë¡œë¡œ ë³€í™˜
    for product in products:
        if "images" in product:
            product["images"] = [
                str(Path(p).relative_to(OUTPUT_DIR)) for p in product["images"]
            ]

    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(products, f, ensure_ascii=False, indent=2)

    print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥: {output_file}")


def main():
    category_name = "ìì¼“"
    category_sno = CATEGORIES[category_name]

    # 1. ìƒí’ˆ ê²€ìƒ‰
    found_products = fetch_products_by_category(category_sno, category_name)

    if not found_products:
        print("ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    # 2. ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ (ìƒ‰ìƒ + ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ)
    enrich_product_details(found_products)

    # 3. ê²°ê³¼ ì €ì¥
    save_results(found_products)

    # 4. ê²°ê³¼ ì¶œë ¥
    print("\n" + "=" * 50)
    print(f"ìµœì¢… ê²°ê³¼: {len(found_products)}ê°œ ìƒí’ˆ")
    print("=" * 50)

    for i, product in enumerate(found_products, 1):
        print(f"\n{i}. {product['name']}")
        print(
            f"   êµ¬ë§¤: {product['sell_count']:,}ê°œ | ë¦¬ë·°: {product['review_count']}ê°œ | ê¸ì •ë¥ : {product['positive_percent']}%"
        )
        print(f"   ìƒ‰ìƒ: {product.get('option_colors', 'N/A')}")
        print(f"   ì´ë¯¸ì§€: {len(product.get('images', []))}ê°œ")
        print(f"   {product['url']}")


if __name__ == "__main__":
    main()
